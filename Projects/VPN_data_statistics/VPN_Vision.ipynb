{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted and unique merged data has been saved to 'output_csv_folder/merged_data_sorted.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# 指定输入和输出文件夹路径\n",
    "input_folder = 'input_csv_folder'\n",
    "output_file = 'merged_data_sorted.csv'\n",
    "\n",
    "# 创建一个集合来存储数据\n",
    "merged_data_unique = set()\n",
    "\n",
    "# 遍历指定文件夹中的所有文件\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "        # 读取每个 CSV 文件中的数据\n",
    "        with open(file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            # 跳过第一行（标题行）\n",
    "            next(reader)\n",
    "            # 将每行数据加入到集合中\n",
    "            for row in reader:\n",
    "                merged_data_unique.add(tuple(row))\n",
    "\n",
    "# 将集合中的数据转换为列表\n",
    "merged_data_list = list(merged_data_unique)\n",
    "\n",
    "# 根据 'log_time' 列进行顺序排序\n",
    "merged_data_sorted = sorted(merged_data_list, key=lambda x: x[4])\n",
    "\n",
    "# 将整理后的数据写入新的 CSV 文件\n",
    "# 设置输出文件夹与文件名\n",
    "output_folder = 'output_csv_folder'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "output_file = os.path.join(output_folder, 'merged_data_sorted.csv')\n",
    "\n",
    "# 写入 CSV 文件\n",
    "with open(output_file, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    # 写入标题行\n",
    "    writer.writerow(['node_name', 'rate', 'origin_traffic', 'traffic', 'log_time'])\n",
    "    # 逐行写入数据\n",
    "    for row in merged_data_sorted:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Sorted and unique merged data has been saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traffic summary has been saved to 'traffic_summary.csv'\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "import operator\n",
    "\n",
    "# Data storage dictionary\n",
    "node_traffic = defaultdict(float)\n",
    "traffic_statistics = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "# Read the CSV file and perform traffic statistics\n",
    "\n",
    "output_folder = 'output_csv_folder'\n",
    "file = os.path.join(output_folder, 'merged_data_sorted.csv')\n",
    "with open(file, newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    # Skip the header\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        # Parse the timestamp\n",
    "        log_time = datetime.strptime(row[4], '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # Extract traffic data and convert to MB\n",
    "        traffic = row[3]\n",
    "        if traffic.endswith('KB'):\n",
    "            traffic_mb = float(traffic[:-2]) / 1024\n",
    "        elif traffic.endswith('MB'):\n",
    "            traffic_mb = float(traffic[:-2])\n",
    "        elif traffic.endswith('GB'):\n",
    "            traffic_mb = float(traffic[:-2]) * 1024\n",
    "        elif traffic.endswith('B'):\n",
    "            traffic_mb = float(traffic[:-1]) / (1024 * 1024)\n",
    "        else:\n",
    "            traffic_mb = 0\n",
    "\n",
    "        # Accumulate traffic for each node and time period\n",
    "        node_traffic[row[0]] += traffic_mb\n",
    "        # Consider only the last 168 hours of data\n",
    "        if log_time >= datetime.now() - timedelta(hours=168):\n",
    "            # Round the timestamp to the nearest three hours\n",
    "            log_time = log_time - timedelta(hours=log_time.hour % 3, minutes=log_time.minute, seconds=log_time.second)\n",
    "            # Accumulate traffic\n",
    "            traffic_statistics[row[0]][log_time] += traffic_mb\n",
    "\n",
    "# Convert the statistics into a sorted list\n",
    "sorted_traffic = sorted(traffic_statistics.items(), key=lambda x: sum(x[1].values()), reverse=True)\n",
    "\n",
    "# Get all unique time periods\n",
    "all_time_periods = set()\n",
    "for node_data in traffic_statistics.values():\n",
    "    all_time_periods.update(node_data.keys())\n",
    "\n",
    "# Sort time periods\n",
    "sorted_time_periods = sorted(all_time_periods)\n",
    "\n",
    "\n",
    "output_folder = 'output_csv_folder'\n",
    "output_file1 = os.path.join(output_folder, 'traffic_summary.csv')\n",
    "\n",
    "# Write the results to a CSV file\n",
    "with open(output_file1, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write the header row\n",
    "    header_row = ['Time Period'] + list(node_traffic.keys())\n",
    "    writer.writerow(header_row)\n",
    "\n",
    "    # Write the traffic for each time period\n",
    "    for time_period in sorted_time_periods:\n",
    "        traffic_values = []\n",
    "        for node_name in node_traffic.keys():\n",
    "            traffic_values.append(f'{traffic_statistics[node_name][time_period]:.2f}' if time_period in traffic_statistics[node_name] else '0.00')\n",
    "        writer.writerow([time_period.strftime('%Y-%m-%d %H:%M')] + traffic_values)\n",
    "\n",
    "print(\"Traffic summary has been saved to 'traffic_summary.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted and filtered traffic summary has been saved to 'sorted_traffic_summary.csv'\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "# Read the traffic summary CSV file\n",
    "\n",
    "\n",
    "traffic_summary = []\n",
    "\n",
    "output_folder = 'output_csv_folder'\n",
    "file = os.path.join(output_folder, 'traffic_summary.csv')\n",
    "\n",
    "with open(file, newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        traffic_summary.append(row)\n",
    "\n",
    "# Calculate total traffic for each node\n",
    "node_traffic_totals = defaultdict(float)\n",
    "for row in traffic_summary[1:]:\n",
    "    for i, traffic in enumerate(row[1:], start=1):\n",
    "        node_traffic_totals[traffic_summary[0][i]] += float(traffic)\n",
    "\n",
    "# Insert a new row after the first row with total traffic for each node\n",
    "total_traffic_row = ['Total Traffic'] + [str(node_traffic_totals[node]) for node in traffic_summary[0][1:]]\n",
    "traffic_summary.insert(1, total_traffic_row)\n",
    "\n",
    "# Filter out nodes with total traffic less than 10MB\n",
    "filtered_nodes = [node for node, total_traffic in node_traffic_totals.items() if total_traffic >= 10]\n",
    "\n",
    "# Rearrange traffic summary based on filtered nodes\n",
    "sorted_traffic_summary = []\n",
    "for col_idx, col_data in enumerate(zip(*traffic_summary)):\n",
    "    if col_idx == 0 or col_data[0] in filtered_nodes:\n",
    "        sorted_traffic_summary.append(list(col_data))\n",
    "\n",
    "# Rearrange traffic summary based on filtered nodes and transpose the matrix\n",
    "sorted_traffic_summary_transposed = []\n",
    "\n",
    "# Transpose the data except for the first row and first column\n",
    "for i, row in enumerate(sorted_traffic_summary):\n",
    "    if i == 0:  # Preserve the first row\n",
    "        sorted_traffic_summary_transposed.append(row)\n",
    "    else:\n",
    "        transposed_row = [row[0]] + [f'{float(value):.2f}' for value in row[1:]]  # Preserve the first column, convert the rest to floats\n",
    "        sorted_traffic_summary_transposed.append(transposed_row)\n",
    "\n",
    "# Write the sorted and filtered traffic summary to a new CSV file\n",
    "output_folder = 'output_csv_folder'\n",
    "output_file2 = os.path.join(output_folder, 'sorted_traffic_summary.csv')\n",
    "\n",
    "with open(output_file2, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerows(zip(*sorted_traffic_summary_transposed))\n",
    "\n",
    "print(\"Sorted and filtered traffic summary has been saved to 'sorted_traffic_summary.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily traffic summary has been saved to 'daily_traffic_summary.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 读取并整理数据\n",
    "daily_traffic = defaultdict(lambda: defaultdict(float))  # 以天为键，存储每天每个节点的流量消耗\n",
    "\n",
    "output_folder = 'output_csv_folder'\n",
    "file = os.path.join(output_folder, 'merged_data_sorted.csv')\n",
    "\n",
    "with open(file, 'r', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # 跳过标题行\n",
    "    for row in reader:\n",
    "        node_name, rate, origin_traffic, traffic, log_time = row\n",
    "        # 解析日期时间并提取日期部分\n",
    "        log_date = datetime.strptime(log_time, '%Y-%m-%d %H:%M:%S').date()\n",
    "        # 处理流量数据\n",
    "        if traffic.endswith('GB'):\n",
    "            traffic_mb = float(traffic[:-2]) * 1024\n",
    "        elif traffic.endswith('MB'):\n",
    "            traffic_mb = float(traffic[:-2])\n",
    "        elif traffic.endswith('KB'):\n",
    "            traffic_mb = float(traffic[:-2]) / 1024\n",
    "        elif traffic.endswith('B'):\n",
    "            traffic_mb = float(traffic[:-1]) / 1024 / 1024\n",
    "        else:\n",
    "            raise ValueError(\"Invalid traffic unit\")\n",
    "\n",
    "        # 累加每天每个节点的流量消耗\n",
    "        daily_traffic[log_date][node_name] += traffic_mb\n",
    "\n",
    "# 创建整合的数据框\n",
    "columns = ['Date', 'Total Traffic(MB)']\n",
    "for i in range(1, 6):\n",
    "    columns.extend([f'Rank-{i}-Node', f'Traffic-{i}(MB)'])\n",
    "\n",
    "merged_data = []\n",
    "\n",
    "# 生成整合的数据\n",
    "for date, node_traffic in daily_traffic.items():\n",
    "    \n",
    "    day_data = [date, round(sum(node_traffic.values()), 2)]\n",
    "\n",
    "    # 获取流量前五的节点\n",
    "    sorted_nodes = sorted(node_traffic.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "    # 按照格式添加节点名称和流量消耗量\n",
    "    for node, traffic in sorted_nodes:\n",
    "        day_data.extend([node, round(traffic, 2)])\n",
    "\n",
    "    # 添加到整合数据中\n",
    "    merged_data.append(day_data)\n",
    "\n",
    "# 将merged_data根据日期倒序排列\n",
    "\n",
    "# merged_data = sorted(merged_data, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# 将整合的数据写入 CSV 文件\n",
    "\n",
    "output_folder = 'output_csv_folder'\n",
    "output_file3 = os.path.join(output_folder, 'daily_traffic_summary.csv')\n",
    "\n",
    "with open(output_file3, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(columns)  # 写入标题行\n",
    "    writer.writerows(merged_data)  # 写入数据\n",
    "\n",
    "print(\"Daily traffic summary has been saved to 'daily_traffic_summary.csv'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "henri_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
