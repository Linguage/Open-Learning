# 反向传播

本章的目标是找到一种有效的技术，用于评估前馈神经网络中误差函数 E(w) 的梯度。我们将看到，可以通过一种局部消息传递方案来实现这一目标，在该方案中，信息通过网络向后传递，这被称为误差反向传播，有时简称为反向传播。

从历史上看，反向传播方程通常会手工推导，然后与前向传播方程一起实现在软件中，这两个步骤都需要时间，并容易出错。然而，现代神经网络软件环境允许以极少的额外工作量计算出几乎任何感兴趣的导数，而无需编写原始网络函数。这个被称为自动微分的想法在现代深度学习中起着关键作用。然而，了解计算是如何执行的是有价值的，这样我们就不会依赖于“黑盒子”软件解决方案。因此，在本章中，我们解释了反向传播的关键概念，并详细探讨了自动微分的框架。需要注意的是，在神经计算文献中，“反向传播”一词被用于多种不同的方式。例如，前馈架构可能被称为反向传播网络。此外，“反向传播”一词有时用于描述神经网络的端到端训练过程，包括梯度下降参数更新。在本书中，我们将“反向传播”专门用于描述在数值导数评估中使用的计算过程，例如对于网络权重和偏差的误差函数梯度。该过程也可以应用于其他重要导数的评估，例如雅可比矩阵和海森矩阵。

## 8.1. 梯度的评估 

我们现在推导出一个通用网络的反向传播算法，该网络具有任意的前馈拓扑结构，任意可微的非线性激活函数，以及广泛类别的误差函数。然后，我们将使用一个简单的分层网络结构进行说明，该结构包含一个带有S型隐藏单元的单层，以及一个平方和误差函数。许多实际感兴趣的误差函数，例如那些由一组独立同分布数据的最大似然定义的误差函数，由一系列项的和组成，每个项对应于训练集中的每个数据点，因此

$$E(\mathbf{w})=\sum_{n=1}^N E_n(\mathbf{w}).$$

在这里，我们将考虑在误差函数中一个这样的项的评估∇En(w)的问题。这可以直接用于随机梯度下降，或者可以在一组训练数据点上累积结果，用于批量或小批量方法。


### 8.1.1 单层网络 

首先考虑一个简单的线性模型，其中输出 $y_k$ 是输入变量 $x_i$ 的线性组合，即 

$$y_k=\sum_iw_{ki}x_i$$

结合一个平方和误差函数，对于特定的输入数据点 n，其形式为 
$$E_n=\frac{1}{2}\sum_k(y_{nk}-t_{nk})^2$$
其中 $y_{nk}=y_{k}(\mathbf{x}_{n},\mathbf{w})$，而 $t_{nk}$ 是相应的目标值。关于权重 $w_{ji}$ 的这个误差函数的梯度由 

$$\begin{aligned}\frac{\partial E_n}{\partial w_{ji}}&=(y_{nj}-t_{nj})x_{ni}.\end{aligned}$$

给出。这可以解释为一个“局部”计算，涉及到与链接 $w_{ji}$ 的输出端相关联的“误差信号” $y_{nj} − t_{nj}$ 与与链接输入端相关联的变量 $x_{ni}$ 的乘积。在5.4.3节中，我们看到了类似的公式如何与 logistic-sigmoid 激活函数以及交叉熵误差函数一起出现，以及与 softmax 激活函数及其相匹配的多元交叉熵误差函数类似。现在我们将看到这个简单的结果如何扩展到更复杂的多层前馈网络设置中。

### 8.1.2 通用前馈网络

一般来说，前馈网络由一组单元组成，每个单元计算其输入的加权和：
$$a_j=\sum_iw_{ji}z_i$$
其中$z_i$是另一个单元或发送到单元j的输入单元的激活，而 $w_j$ 是与该连接相关联的权重。可以通过引入一个额外的单元或输入，并将其激活固定为+1，将偏置包含在此总和中，因此我们不需要显式处理偏置。式（8.5）中的总和，称为预激活，在通过非线性激活函数$h(·)$转换为给出单元$j$的激活$z_j$的形式
$$z_j = h(a_j)$$
时进行转换。注意，式（8.5）中总和中的一个或多个变量$z_i$可以是输入，同样，式（8.6）中的单元$j$可以是输出。对于训练集中的每个数据点，我们假设我们已向网络提供了相应的输入向量，并通过连续应用式（8.5）和式（8.6）计算了网络中所有隐藏单元和输出单元的激活。这个过程被称为前向传播，因为它可以被看作是信息通过网络的前向流动。现在考虑对权重 $w_{ji}$ 的 $E_n$ 导数的评估。各个单元的输出将取决于特定的输入数据点 $n$。然而，为了保持符号的简洁，我们将在网络变量中省略下标 $n$ 。首先注意到 $E_n$ 仅通过单元j的总和输入 $a_j$ 依赖于权重 $w_{ji}$。因此，我们可以应用偏导数的链式法则给出
$$\frac{\partial E_{n}}{\partial w_{ji}}=\frac{\partial E_{n}}{\partial a_{j}}\frac{\partial a_{j}}{\partial w_{ji}}.$$
我们现在引入一个有用的符号：
$$\delta_j\equiv\frac{\partial E_n}{\partial a_j}$$


